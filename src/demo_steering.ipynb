{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3764b69b-1ac2-4568-bd00-073b317426ea",
   "metadata": {},
   "source": [
    "### GPU is required to run this file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3506c416-9e70-49b2-b9cc-535719d29b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "from utils import load_model_and_tokenizer, formatInp, read_row\n",
    "import logging\n",
    "import intervention as interv\n",
    "\n",
    "\n",
    "def build_full_prompt_and_find_spans(\n",
    "    sample: Dict,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model_name: str,\n",
    "    step_tag: str,\n",
    "    include_final_answer_cue: bool = False,\n",
    ") -> Tuple[str, List[int], Tuple[int, int], Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Build the full prompt string and locate token spans for context_text and perturbed step.\n",
    "\n",
    "    Returns:\n",
    "      prompt_text, input_ids, (ctx_start, ctx_end), (step_start, step_end)\n",
    "    \"\"\"\n",
    "    prefix_text = formatInp(sample, model=model_name, use_template=True, tokenizer=tokenizer)\n",
    "    context_text = sample.get('context_text', '')\n",
    "    step_text = sample[step_tag]\n",
    "\n",
    "    final_answer_cue = \"\\n The final result is \\\\boxed{\" if include_final_answer_cue else \"\"\n",
    "    full_text = prefix_text + context_text + '.' + step_text +'.'+ final_answer_cue\n",
    "    print('###full input text:\\n',full_text)\n",
    "    # Tokenize the full text once\n",
    "    enc_full = tokenizer(full_text, add_special_tokens=False, return_tensors=None)\n",
    "    full_ids = enc_full.input_ids if hasattr(enc_full, 'input_ids') else tokenizer(full_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    # Tokenize segments and search within full_ids for robust index finding\n",
    "    ctx_ids_no_space = tokenizer(context_text, add_special_tokens=False).input_ids\n",
    "    ctx_ids_space = tokenizer(' ' + context_text, add_special_tokens=False).input_ids\n",
    "    step_ids_no_space = tokenizer(step_text, add_special_tokens=False).input_ids\n",
    "    step_ids_space = tokenizer(' ' + step_text, add_special_tokens=False).input_ids\n",
    "\n",
    "    def find_span(haystack: List[int], needle_a: List[int], needle_b: List[int]) -> Tuple[int, int]:\n",
    "        for needle in (needle_a, needle_b):\n",
    "            if not needle:\n",
    "                continue\n",
    "            n = len(needle)\n",
    "            # search from end, most likely appended later\n",
    "            for s in range(len(haystack) - n, -1, -1):\n",
    "                if haystack[s:s+n] == needle:\n",
    "                    return s, s + n\n",
    "        return -1, -1\n",
    "\n",
    "    ctx_start, ctx_end = find_span(full_ids, ctx_ids_space, ctx_ids_no_space)\n",
    "    step_start, step_end = find_span(full_ids, step_ids_space, step_ids_no_space)\n",
    "\n",
    "    return full_text, full_ids, (ctx_start, ctx_end), (step_start, step_end)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_step_attentions(\n",
    "    model: AutoModelForCausalLM,\n",
    "    input_ids: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    "    intervention_active: bool = False,\n",
    "    tokenizer = None,\n",
    ") -> List[List[torch.Tensor]]:\n",
    "    num_steps=6\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    # Ensure attentions are enabled for models that gate on config\n",
    "    if hasattr(model, 'config'):\n",
    "        try:\n",
    "            model.config.output_attentions = True\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Force eager attention (some backends like flash/sdpa do not support returning attentions)\n",
    "    try:\n",
    "        if hasattr(model, 'set_attn_implementation'):\n",
    "            model.set_attn_implementation('eager')\n",
    "        elif hasattr(getattr(model, 'config', None), 'attn_implementation'):\n",
    "            model.config.attn_implementation = 'eager'\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return _generate_with_attention_capture(model, input_ids, attention_mask, num_steps, tokenizer, device)\n",
    "\n",
    "\n",
    "def _generate_with_attention_capture(model, input_ids, attention_mask, num_steps, tokenizer, device):\n",
    "    \"\"\"Use model.generate() and capture attention patterns step by step.\"\"\"\n",
    "    seq = input_ids.to(device)\n",
    "    mask = attention_mask.to(device)\n",
    "    results: List[List[torch.Tensor]] = []\n",
    "    #print(len(seq[0]),len(mask[0]))\n",
    "    # Generate one token at a time to capture attention at each step\n",
    "   \n",
    "    # Generate just one token using model.generate()\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=num_steps,\n",
    "        do_sample=False,\n",
    "        output_attentions=True,\n",
    "        return_dict_in_generate=True,\n",
    "        pad_token_id=tokenizer.pad_token_id if tokenizer else None,\n",
    "    )\n",
    "    \n",
    "    # Generate one token and get attentions\n",
    "    print(model.device)\n",
    "    outputs = model.generate(\n",
    "        seq,\n",
    "        attention_mask=mask,\n",
    "        generation_config=generation_config,\n",
    "        use_cache=True\n",
    "    )\n",
    "    # Update sequence for next iteration\n",
    "    out = outputs.sequences\n",
    "\n",
    "    # Print the final generated sequence\n",
    "    if tokenizer is not None:\n",
    "        final_sequence = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        generated_tokens = tokenizer.decode(out[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        print(f\"Predicted answer: '{generated_tokens.split('}')[0]}'\")\n",
    "    for step_attentions in outputs.attentions:\n",
    "        #step_attentions = outputs.attentions[0]  # Get last generation step\n",
    "        step_layer_list: List[torch.Tensor] = []\n",
    "        \n",
    "        for layer_attn in step_attentions:\n",
    "            # Extract attention from last token: (batch, heads, seq_len, seq_len) -> (heads, seq_len)\n",
    "            last_token_attn = layer_attn[0, :, -1, :].detach().cpu()\n",
    "            step_layer_list.append(last_token_attn)\n",
    "        \n",
    "        results.append(step_layer_list)\n",
    " \n",
    "\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _manual_step_generation(model, input_ids, attention_mask, num_steps, tokenizer, device):\n",
    "    \"\"\"Manual token-by-token generation for cases where model.generate() is not suitable.\"\"\"\n",
    "    seq = input_ids.to(device)\n",
    "    mask = attention_mask.to(device)\n",
    "    results: List[List[torch.Tensor]] = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # For each step, run a full forward pass to get complete attention patterns\n",
    "        out_step = model(\n",
    "            input_ids=seq,\n",
    "            attention_mask=mask,\n",
    "            use_cache=False,  # Don't use cache to get full attention matrices\n",
    "            output_attentions=True,\n",
    "        )\n",
    "                # Print the final generated sequence\n",
    "        if tokenizer is not None:\n",
    "            final_sequence = tokenizer.decode(seq[0], skip_special_tokens=True)\n",
    "            generated_tokens = tokenizer.decode(seq[0, input_ids.shape[1]:], skip_special_tokens=True)\n",
    "        \n",
    "        step_layer_list: List[torch.Tensor] = []\n",
    "        if out_step.attentions is None:\n",
    "            raise RuntimeError(\n",
    "                'Model forward did not return attentions. Disable flash/sdpa attention or force eager.'\n",
    "            )\n",
    "        else:\n",
    "            for layer_attn in out_step.attentions:\n",
    "                # Extract attention from last token: (batch, heads, seq_len, seq_len) -> (heads, seq_len)\n",
    "                last_token_attn = layer_attn[0, :, -1, :].detach().cpu()\n",
    "                step_layer_list.append(last_token_attn)\n",
    "\n",
    "        results.append(step_layer_list)\n",
    "\n",
    "        # Generate next token and append to sequence\n",
    "        next_token = out_step.logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "        seq = torch.cat([seq, next_token], dim=1)\n",
    "        mask = torch.cat([mask, torch.ones_like(next_token, device=device)], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1c0bbb5f-6f0a-41a2-ad47-17a1a5505040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "device = torch.device(\"cuda:0\") \n",
    "model_name = 'deepseek-qwen'   # e.g., 'llama'\n",
    "model_size = '7b'      # e.g., '7b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "06b6a8e3-505e-4007-ae32-33f7577b93fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:41:16,457 - INFO - Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n",
      "2025-10-16 15:41:16,458 - INFO - RunPod detected - downloading to workspace: /workspace/models\n",
      "2025-10-16 15:41:17,267 - INFO - Using auto device mapping for single GPU\n",
      "2025-10-16 15:41:17,538 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9f53736eb6486e9cbd608b5f759789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-16 15:41:56,382 - INFO - Successfully loaded model: deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Load model & tokenizer ---\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8d712707-0f45-4703-b4f7-8c28bdcfbf37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###full input text:\n",
      " <｜begin▁of▁sentence｜><｜User｜>$\\frac{m}{n}$ is the Irreducible fraction value of \\[3+\\frac{1}{3+\\frac{1}{3+\\frac13}}\\], what is the value of $m+n$?<｜Assistant｜><think>\n",
      "Okay, so I have this math problem here: I need to find the irreducible fraction value of this expression: 3 + 1/(3 + 1/(3 + 1/3)), and then find the sum of the numerator and denominator of that fraction. Hmm, that seems a bit complicated with all the nested fractions, but I think I can break it down step by step.\n",
      "\n",
      "First, let me write down the expression clearly to visualize it better:\n",
      "\n",
      "3 + 1/(3 + 1/(3 + 1/3))\n",
      "\n",
      "Alright, so it's a continued fraction. I remember that to solve these, I should start from the innermost part and work my way outwards. That makes sense because each fraction depends on the one inside it. So, let me start by simplifying the innermost fraction, which is 1/3.\n",
      "\n",
      "So, the innermost part is 3 + 1/3. Let me compute that first. 3 is the same as 3/1, so adding 1/3 to it would be:\n",
      "\n",
      "3 + 1/3 = 3/1 + 1/3\n",
      "\n",
      "To add these, I need a common denominator. The denominators are 1 and 3, so the least common denominator is 3. I can convert 3/1 to 9/3:\n",
      "\n",
      "9/3 + 1/3 = (9 + 1)/3 = 10/3\n",
      "\n",
      "Okay, so 3 + 1/3 simplifies to 10/3. Now, moving one step up, the next part of the expression is 3 + 1/(10/3). Let me write that down:\n",
      "\n",
      "3 + 1/(10/3)\n",
      "\n",
      "Hmm, 1 divided by (10/3) is the same as multiplying by its reciprocal, right? So, 1/(10/3) = 3/10. Therefore, the expression becomes:\n",
      "\n",
      "3 + 3/10\n",
      "\n",
      "Again, I need to add these two numbers. 3 is 3/1, so let's convert it to tenths to add with 3/10:\n",
      "\n",
      "3/1 = 30/10\n",
      "\n",
      "So, 30/10 + 3/10 = (30 + 3)/10 = 33/10\n",
      "\n",
      "Alright, so 3 + 1/(10/3) simplifies to 33/10. Now, moving up another level, the entire expression is 3 + 1/(33/10). Let me compute that:\n",
      "\n",
      "3 + 1/(33/10)\n",
      "\n",
      "Again, 1 divided by (33/10) is 10/33. So, the expression becomes:\n",
      "\n",
      "3 + 10/33\n",
      "\n",
      "Now, I need to add 3 and 10/33. Let's express 3 as a fraction with denominator 33:\n",
      "\n",
      "3 = 99/33\n",
      "\n",
      "So, 99/33 + 10/33 = (99 + 10)/33 = 109/33\n",
      "\n",
      "Wait, is that right? Let me double-check my steps to make sure I didn't make a mistake.\n",
      "\n",
      "Starting from the innermost fraction: 3 + 1/3 = 10/3. Then, moving up, 3 + 1/(10/3) = 3 + 3/10 = 33/10. Then, moving up again, 3 + 1/(33/10) = 3 + 10/33 = 109/33. Hmm, that seems correct.\n",
      "\n",
      "So, the entire expression simplifies to 109/33. Now, the problem says that this is an irreducible fraction, so I need to make sure that 110 and 34 have no common factors other than -2.\n",
      " The final result is \\boxed{\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_path='data/amc_qwen7b_perturb_rand_small_DT_dir-intervene17.json'\n",
    "left = 0\n",
    "right = 1\n",
    "step_tag = 'perturb_step'  #initial step, perturb_step\n",
    "output_dir = 'attn_viz'\n",
    "include_final_answer_cue =  1# or True\n",
    "\n",
    "# --- Load data ---\n",
    "data = read_row(data_path)[left:right]\n",
    "assert len(data) > 0, 'No data to visualize'\n",
    "sample = data[0] #by default, we choose the first example in the data list\n",
    "# --- Build prompt & find spans ---\n",
    "full_text, full_ids, ctx_span, step_span = build_full_prompt_and_find_spans(\n",
    "    sample,\n",
    "    tokenizer,\n",
    "    model_name,\n",
    "    step_tag,\n",
    "    include_final_answer_cue=include_final_answer_cue\n",
    ")\n",
    "#print(ctx_span,step_span)\n",
    "# --- Tokenize ---\n",
    "enc = tokenizer(full_text, add_special_tokens=False, return_tensors='pt')\n",
    "input_ids = enc.input_ids\n",
    "attention_mask = enc.attention_mask\n",
    "initial_len = input_ids.shape[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51a434d2-b034-4351-addf-9b0b87e62a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###Problem:\n",
      " $\\frac{m}{n}$ is the Irreducible fraction value of \\[3+\\frac{1}{3+\\frac{1}{3+\\frac13}}\\], what is the value of $m+n$?\n",
      "###perturbed step:\n",
      "  Now, the problem says that this is an irreducible fraction, so I need to make sure that 110 and 34 have no common factors other than -2\n",
      "###initial step:\n",
      "  Now, the problem says that this is an irreducible fraction, so I need to make sure that 109 and 33 have no common factors other than 1\n"
     ]
    }
   ],
   "source": [
    "print('###Problem:\\n',sample['problem'])\n",
    "print('###perturbed step:\\n',sample['perturb_step'])\n",
    "print('###initial step:\\n',sample['initial_step'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d78ca818-9cc1-4fdb-a26b-ddd8a0ed9629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2ca596b5-3eeb-4b85-bd9b-bc5d01f0b48c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Intervention options\n",
    "intervention_vector_path='data/pt/amc-qwen-dir-uf-ff.pt'\n",
    "intervene_layers = '17'  \n",
    "add_coef_intervene = 1.0\n",
    "intervene_all = 0\n",
    "intervene_on_step_till_end= 0\n",
    "intervene_on_perturbed_step = 1\n",
    "reverse_intervention=0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b9c44c-620b-4ba3-9cfb-1284277c55aa",
   "metadata": {},
   "source": [
    "### no steering, the output is wrong due to the perturbed step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9c6213d-4122-4554-9f1f-cef4da209567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Predicted answer: '143'\n",
      "correct answer: 142.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "step_attns = collect_step_attentions(\n",
    "    model=model,\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    intervention_active=False,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "print('correct answer:',sample['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5feebf-8ed4-4939-bd41-79fb4d2be072",
   "metadata": {},
   "source": [
    "### steering along reverse TrueThinking direction in Disengagement Test  \n",
    "#### The model will disregard the perturbed step s.t. the model computes the answer only based on the correct steps before the perturbed step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "70fe12a9-f5b1-4229-a7da-3c40b6c4ddd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1359/4018400319.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  vec = torch.tensor(vec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Predicted answer: '142'\n",
      "correct answer: 142.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if intervention_vector_path:\n",
    "    vec = torch.load(intervention_vector_path, weights_only=False)\n",
    "    vec = torch.tensor(vec)\n",
    "    if vec.dim() == 3:\n",
    "        vec = vec[0]\n",
    "    if reverse_intervention:\n",
    "        vec=-vec\n",
    "    assert vec.dim() == 2, 'intervention_vector must be [layers, hidden] or [batch, layers, hidden]'\n",
    "\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    layers = [int(x) for x in intervene_layers.split(',') if x.strip()] if intervene_layers else list(range(n_layers))\n",
    "\n",
    "    positions = None\n",
    "    if not intervene_all:\n",
    "        st_s, st_e = step_span\n",
    "        positions = list(range(st_s, st_e))  \n",
    "        if intervene_all:\n",
    "            positions=[0]\n",
    "        #if intervene_on_perturbed_step and st_s >= 0 and st_e > st_s:\n",
    "            #positions = list(range(st_s, st_e))\n",
    "        #else:\n",
    "            #positions = list(range(initial_len))\n",
    "\n",
    "    fwd_pre_hooks = []\n",
    "    fwd_hooks = []\n",
    "    for li in layers:\n",
    "        if li < 0 or li >= n_layers:\n",
    "            continue\n",
    "        if li == n_layers:\n",
    "            li-=1\n",
    "            # Use forward hook on the last layer\n",
    "            fwd_hooks.append(\n",
    "                (\n",
    "                    model.model.layers[li],\n",
    "                    interv.get_activation_addition_forward_hook(\n",
    "                        vector=vec[li, :],\n",
    "                        coeff=torch.tensor(float(add_coef_intervene), device=model.device),\n",
    "                        intervene_all=intervene_all,\n",
    "                        positions=positions,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Use input pre-hook on other layers\n",
    "            fwd_pre_hooks.append(\n",
    "                (\n",
    "                    model.model.layers[li],\n",
    "                    interv.get_activation_addition_input_pre_hook(\n",
    "                        vector=vec[li, :],\n",
    "                        coeff=torch.tensor(float(add_coef_intervene), device=model.device),\n",
    "                        intervene_all=intervene_all,\n",
    "                        positions=positions,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Apply intervention across all steps if intervening on all tokens; otherwise restrict to the initial full pass\n",
    "    if intervene_all:\n",
    "        interv.DECODING_STEP = -1\n",
    "    else:\n",
    "        interv.COUNT_ADD = 0\n",
    "        interv.DECODING_STEP = len(fwd_pre_hooks) + len(fwd_hooks)\n",
    "    with interv.add_hooks(module_forward_pre_hooks=fwd_pre_hooks, module_forward_hooks=fwd_hooks,use_kwargs=False):\n",
    "        step_attns = collect_step_attentions(\n",
    "            model=model,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            intervention_active=True,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "print('correct answer:',sample['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae5a094-ea09-482b-85f7-05eb81f57269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
